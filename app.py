# app.py (offline-first)
# Robot TCR GTQ/USD ‚Äî Banguat + Pron√≥stico
# Carga primero data/banguat_tcr_gtq_usd.csv (versi√≥n controlada en GitHub).
# Si no existe, intenta descargar via WS en memoria (sin persistir).

import os
import re
import datetime as dt

import requests
from requests.adapters import HTTPAdapter
from urllib3.util.retry import Retry

import numpy as np
import pandas as pd
import streamlit as st
from bs4 import BeautifulSoup
from lxml import etree
from dateutil.relativedelta import relativedelta
import matplotlib.pyplot as plt  # <- para las figuras

# ----- Modelos -----
USE_PROPHET = True
try:
    from prophet import Prophet
except Exception:
    USE_PROPHET = False

USE_ARIMA = True
try:
    import statsmodels.api as sm
except Exception:
    USE_ARIMA = False

st.set_page_config(page_title="Robot TCR GTQ/USD ‚Äî Offline-first", page_icon="üí±", layout="wide")
st.title("üí± Robot del Tipo de Cambio (GTQ/USD) ‚Äî Banguat + Pron√≥stico (offline-first)")
st.caption("Usa el CSV del repositorio como fuente principal. WS solo como respaldo.")

# ---------- UI (sidebar) ----------
with st.sidebar:
    st.header("Consulta")
    sel_date = st.date_input("Selecciona una fecha:", value=dt.date.today(),
                             min_value=dt.date(1998,1,1),
                             max_value=dt.date.today() + relativedelta(years=5))
    model_choice = st.radio("Modelo (si es futuro):",
                            ["ARIMA (alterno)", "Random Walk (baseline)"] + (["Prophet (opcional)"] if USE_PROPHET else []),
                            index=0)
    st.divider()
    st.subheader("Datos")
    st.write("Fuente prioritaria: `data/banguat_tcr_gtq_usd.csv` (en el repo).")
    try_ws = st.checkbox("Si no hay CSV, intentar WS de Banguat", value=True)

# ---------- Utilidades ----------
def _requests_session():
    s = requests.Session()
    retry = Retry(total=5, connect=5, read=5, backoff_factor=0.5,
                  status_forcelist=[500,502,503,504], allowed_methods=["GET","POST"])
    s.mount("https://", HTTPAdapter(max_retries=retry))
    s.mount("http://", HTTPAdapter(max_retries=retry))
    return s

SESSION = _requests_session()
BANGUAT_SOAP_URL = "https://www.banguat.gob.gt/variables/ws/TipoCambio.asmx"
SOAP_HEADERS = {"Content-Type":"text/xml; charset=utf-8"}

def _soap_post(body_xml: str, soap_action: str, timeout=30):
    headers = SOAP_HEADERS.copy()
    headers["SOAPAction"] = f"http://www.banguat.gob.gt/variables/ws/{soap_action}"
    resp = SESSION.post(BANGUAT_SOAP_URL, data=body_xml.encode("utf-8"), headers=headers, timeout=timeout)
    resp.raise_for_status(); return resp.content

def _soap_envelope_rango(fini:str, ffin:str, variant:int=0) -> str:
    if variant == 0:
        params = f"<fechainit>{fini}</fechainit><fechafin>{ffin}</fechafin>"
    elif variant == 1:
        params = f"<fecha_ini>{fini}</fecha_ini><fecha_fin>{ffin}</fecha_fin>"
    else:
        params = f"<fechaini>{fini}</fechaini><fechafin>{ffin}</fechafin>"
    return f"""<?xml version="1.0" encoding="utf-8"?>
<soap:Envelope xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"
               xmlns:xsd="http://www.w3.org/2001/XMLSchema"
               xmlns:soap="http://schemas.xmlsoap.org/soap/envelope/">
  <soap:Body>
    <TipoCambioRango xmlns="http://www.banguat.gob.gt/variables/ws/">
      {params}
    </TipoCambioRango>
  </soap:Body>
</soap:Envelope>""".strip()

def _parse_fecha_referencia(xml_bytes: bytes) -> pd.DataFrame:
    root = etree.fromstring(xml_bytes)
    nodes = root.xpath('//*[local-name()="Var" or local-name()="VarDolar"]')
    fechas, refs = [], []
    for n in nodes:
        fecha_nodes = n.xpath('./*[local-name()="fecha"]/text()')
        ref_nodes   = n.xpath('./*[local-name()="referencia"]/text()')
        if fecha_nodes and ref_nodes:
            fechas.append(fecha_nodes[0]); refs.append(ref_nodes[0])
    if not fechas:
        all_fecha = root.xpath('//*[local-name()="fecha"]/text()')
        all_ref   = root.xpath('//*[local-name()="referencia"]/text()')
        if all_fecha and all_ref and len(all_fecha)==len(all_ref): fechas, refs = all_fecha, all_ref
    if not fechas: return pd.DataFrame(columns=["date","tcr"])
    def parse_date(s):
        try: return pd.to_datetime(s, format="%d/%m/%Y").date()
        except: return None
    date_list = [parse_date(s) for s in fechas]
    tcr_list = []
    for s in refs:
        try: tcr_list.append(float(str(s).replace(",",".")))
        except: tcr_list.append(np.nan)
    df = pd.DataFrame({"date":date_list,"tcr":tcr_list}).dropna().sort_values("date").reset_index(drop=True)
    return df

def _normalize_daily(df: pd.DataFrame) -> pd.DataFrame:
    df = df.drop_duplicates("date").set_index("date").sort_index()
    full_idx = pd.date_range(df.index.min(), dt.date.today(), freq="D").date
    s = df["tcr"].reindex(full_idx).ffill()
    return pd.DataFrame({"date": full_idx, "tcr": s.values})

@st.cache_data(ttl=60*60*12)
def fetch_ws_memory(start_date=dt.date(1998,1,1)) -> pd.DataFrame:
    parts = []
    fini = start_date; ffin = dt.date.today()
    step = dt.timedelta(days=120)
    while fini <= ffin:
        end = min(fini + step, ffin)
        fini_s, end_s = fini.strftime("%d/%m/%Y"), end.strftime("%d/%m/%Y")
        got = None
        for variant in (0,1,2):
            try:
                body = _soap_envelope_rango(fini_s, end_s, variant)
                xml_bytes = _soap_post(body, "TipoCambioRango", timeout=40)
                tmp = _parse_fecha_referencia(xml_bytes)
                if not tmp.empty: got = tmp; break
            except Exception:
                continue
        if got is not None:
            parts.append(got)
        fini = end + dt.timedelta(days=1)
    if parts:
        df = pd.concat(parts, ignore_index=True)
        df = df.drop_duplicates("date").sort_values("date").reset_index(drop=True)
        return _normalize_daily(df)
    return pd.DataFrame(columns=["date","tcr"])

# ---------- Carga de datos (CSV primero) ----------
hist = None
csv_path = os.path.join("data","banguat_tcr_gtq_usd.csv")
if os.path.exists(csv_path):
    hist = pd.read_csv(csv_path)
    hist["date"] = pd.to_datetime(hist["date"]).dt.date
    hist = _normalize_daily(hist)
    source_label = "CSV del repositorio"
else:
    if try_ws:
        with st.spinner("No hay CSV. Intentando WS de Banguat (modo memoria)..."):
            hist = fetch_ws_memory(dt.date(1998,1,1))
        source_label = "WS (memoria)"
    else:
        hist = pd.DataFrame(columns=["date","tcr"])
        source_label = "Sin datos"

if hist.empty:
    st.error("No hay datos. Agrega `data/banguat_tcr_gtq_usd.csv` al repositorio o activa WS.")
    st.stop()

# ---------- Resultado ----------
last_date = hist["date"].max()
is_future = sel_date > last_date

colA, colB, colC = st.columns([2,1,1])
with colA: st.markdown(f"**Fuente:** {source_label}")
with colB: st.metric("√öltimo d√≠a con dato", str(last_date))
with colC: st.metric("Observaciones", f"{len(hist):,}")

st.markdown("### Resultado")
if not is_future:
    val = hist.set_index("date").loc[sel_date, "tcr"]
    st.success(f"**TCR oficial para {sel_date}: Q {val:.5f}**")

else:
    # --- PRON√ìSTICO ---
    y = hist["tcr"].astype("float64")
    horizon = (sel_date - last_date).days
    try:
        if model_choice.startswith("ARIMA") and USE_ARIMA:
            model = sm.tsa.SARIMAX(y, order=(1,1,0),
                                   enforce_stationarity=False,
                                   enforce_invertibility=False).fit(disp=False)
            pred = model.get_forecast(steps=horizon)
            yhat = float(pred.predicted_mean.iloc[-1])
            lo, hi = pred.conf_int(alpha=0.2).iloc[-1]
            model_used = "ARIMA(1,1,0)"
        elif model_choice.startswith("Prophet") and USE_PROPHET:
            m = Prophet(growth="flat", daily_seasonality=True,
                        weekly_seasonality=False, yearly_seasonality=True,
                        interval_width=0.8)
            m.fit(hist.rename(columns={"date":"ds","tcr":"y"}))
            future = m.make_future_dataframe(periods=horizon, freq="D", include_history=True)
            fcst = m.predict(future)
            row = fcst.loc[fcst["ds"] == pd.Timestamp(sel_date)]
            if row.empty: row = fcst.iloc[[-1]]
            yhat = float(row["yhat"]); lo = float(row["yhat_lower"]); hi = float(row["yhat_upper"])
            model_used = "Prophet"
        else:
            yhat = y.iloc[-1]; lo, hi = yhat-0.05, yhat+0.05; model_used = "Random Walk"
    except Exception:
        yhat = y.iloc[-1]; lo, hi = yhat-0.05, yhat+0.05; model_used = "Random Walk"

    st.info(f"**Pron√≥stico para {sel_date}** (modelo **{model_used}**): "
            f"**Q {yhat:.5f}**  _IC 80%: {lo:.5f} ‚ü∑ {hi:.5f}_")

    # === FIGURA 2: cola (24 meses) + pron√≥stico con banda ===
    win_months = 24
    h = hist.copy()
    h["date"] = pd.to_datetime(h["date"])
    cut = pd.Timestamp(last_date) - pd.DateOffset(months=win_months)
    h = h[h["date"] >= cut]

    steps = max(1, (sel_date - last_date).days)
    if model_used.startswith("ARIMA") and 'model' in locals():
        pred = model.get_forecast(steps=steps)
        f_dates = pd.date_range(last_date + pd.Timedelta(days=1), sel_date, freq="D")
        f_mean = pred.predicted_mean.to_numpy()
        ci = pred.conf_int(alpha=0.2).to_numpy()  # 80%
        f_lo, f_hi = ci[:, 0], ci[:, 1]

    elif model_used.startswith("Prophet") and 'fcst' in locals():
        f = fcst[fcst["ds"] > pd.Timestamp(last_date)]
        f_dates = pd.to_datetime(f["ds"])
        f_mean = f["yhat"].to_numpy()
        f_lo = f["yhat_lower"].to_numpy()
        f_hi = f["yhat_upper"].to_numpy()

    else:
        # Random Walk: l√≠nea horizontal (√∫ltimo valor) y banda constante
        f_dates = pd.date_range(last_date + pd.Timedelta(days=1), sel_date, freq="D")
        last_val = float(h["tcr"].iloc[-1])
        f_mean = np.full(len(f_dates), last_val)
        f_lo   = np.full(len(f_dates), float(lo))
        f_hi   = np.full(len(f_dates), float(hi))

    fig2, ax2 = plt.subplots(figsize=(8,3))
    ax2.plot(h["date"], h["tcr"], label="Hist√≥rico", linewidth=1.4)
    ax2.plot(f_dates, f_mean, label="Pron√≥stico", linewidth=1.6)
    ax2.fill_between(f_dates, f_lo, f_hi, alpha=0.20, label="IC 80%")
    ax2.axvline(pd.Timestamp(last_date), linestyle="--", linewidth=1, alpha=0.6)
    ax2.scatter([pd.Timestamp(sel_date)],
                [f_mean[-1] if len(f_mean) else float(yhat)],
                s=30, zorder=3, label=f"{sel_date}")
    ax2.set_xlabel("Fecha"); ax2.set_ylabel("Q por USD")
    ax2.grid(True, alpha=0.25); ax2.legend(loc="best")

    st.pyplot(fig2, clear_figure=True)
    fig2.savefig("fig_pronostico.png", dpi=300, bbox_inches="tight")

# ---------- Figura 1: hist√≥rico completo ----------
fig, ax = plt.subplots(figsize=(10,4))
ax.plot(pd.to_datetime(hist["date"]), hist["tcr"], linewidth=1.5, label="Hist√≥rico")
ax.set_xlabel("Fecha"); ax.set_ylabel("Q por USD"); ax.grid(True, alpha=0.25)
ax.legend(loc="best")
st.pyplot(fig, clear_figure=True)
fig.savefig("fig_hist.png", dpi=300, bbox_inches="tight")

# ---------- Descarga ----------
st.download_button("‚¨áÔ∏è Descargar CSV (hist√≥rico en uso)", data=hist.to_csv(index=False).encode("utf-8"),
                   file_name="banguat_tcr_gtq_usd.csv", mime="text/csv")

with st.expander("Notas"):
    st.markdown("- Este modo usa **CSV versionado**. Para mantenerlo actualizado, configura el **workflow de GitHub Actions** incluido en el repo.")
    
# =========================================================
# === BLOQUE NUEVO: DESARROLLO 95% (tramos + back-testing)
# =========================================================
import numpy as np
import pandas as pd
import streamlit as st
from statsmodels.stats.stattools import jarque_bera

st.markdown("---")
st.header("Desarrollo 95% (tramos + back-testing)")

# ---------- Helpers de modelado ----------
def fit_poly(x, y, deg):
    X = np.vstack([x**k for k in range(deg+1)]).T
    beta = np.linalg.lstsq(X, y, rcond=None)[0]
    yhat = X @ beta
    ss_res = float(np.sum((y - yhat)**2))
    ss_tot = float(np.sum((y - y.mean())**2))
    r2 = 1 - ss_res/ss_tot if ss_tot>0 else 0.0
    return yhat, r2, {"beta": beta}

def fit_loglin(x, y):
    # y > 0 para usar log
    y_safe = np.where(y <= 0, np.nan, y)
    mask = ~np.isnan(y_safe)
    if mask.sum() < 3:
        return np.full_like(y, y.mean(), dtype=float), 0.0, {"ok": False}
    Y = np.log(y_safe[mask])
    X = np.vstack([np.ones(mask.sum()), x[mask]]).T
    beta = np.linalg.lstsq(X, Y, rcond=None)[0]
    yhat = np.exp(beta[0] + beta[1]*x)
    ss_res = float(np.nansum((y - yhat)**2))
    ss_tot = float(np.nansum((y - np.nanmean(y))**2))
    r2 = 1 - ss_res/ss_tot if ss_tot>0 else 0.0
    return yhat, r2, {"beta": beta, "ok": True}

def moving_average(y, win=30):
    return pd.Series(y).rolling(win, min_periods=1).mean().values

def fit_ses(y):
    # SES sin dependencias extra (wrapper simple de Holt-Winters si la tuvieras),
    # aqu√≠ usamos la media m√≥vil como baseline SES si no hay stats especial.
    # Si prefieres, deja este SES simple; suele bastar para TCR cuasi plano.
    # Ajuste "alpha" por b√∫squeda en malla para minimizar RMSE in-sample.
    best = (np.inf, 0.2, None)  # (rmse, alpha, yhat)
    for alpha in np.linspace(0.05, 0.95, 19):
        yhat = np.zeros_like(y, dtype=float)
        yhat[0] = y[0]
        for t in range(1, len(y)):
            yhat[t] = alpha*y[t-1] + (1-alpha)*yhat[t-1]
        rmse = float(np.sqrt(np.mean((y - yhat)**2)))
        if rmse < best[0]:
            best = (rmse, alpha, yhat.copy())
    yhat = best[2]
    ss_res = float(np.sum((y - yhat)**2))
    ss_tot = float(np.sum((y - y.mean())**2))
    r2 = 1 - ss_res/ss_tot if ss_tot>0 else 0.0
    return yhat, r2, {"alpha": best[1]}

def pearson_r(y, yhat):
    y1 = y - y.mean(); y2 = yhat - yhat.mean()
    denom = float(np.sqrt((y1**2).sum() * (y2**2).sum()))
    return float((y1*y2).sum()/denom) if denom>0 else 0.0

def metrics_test(y_true, y_pred):
    err = y_true - y_pred
    mae = float(np.mean(np.abs(err)))
    rmse = float(np.sqrt(np.mean(err**2)))
    mape = float(np.mean(np.abs(err / y_true)) * 100)
    rmse_pct = float(rmse / max(1e-9, np.mean(y_true)) * 100)
    jb_stat, jb_p, _, _ = jarque_bera(err)
    return mae, rmse, mape, rmse_pct, float(jb_p)

def choose_model(x_tr, y_tr, allow):
    cands = []
    # Poly1/2/3
    if allow["poly1"]:
        y1, r2_1, p1 = fit_poly(x_tr, y_tr, 1); cands.append(("Poly1", y1, r2_1, p1))
    if allow["poly2"]:
        y2, r2_2, p2 = fit_poly(x_tr, y_tr, 2); cands.append(("Poly2", y2, r2_2, p2))
    if allow["poly3"]:
        y3, r2_3, p3 = fit_poly(x_tr, y_tr, 3); cands.append(("Poly3", y3, r2_3, p3))
    # Log-lineal
    if allow["log"]:
        yl, r2_l, pl = fit_loglin(x_tr, y_tr); cands.append(("Log-lin", yl, r2_l, pl))
    # SES
    if allow["ses"]:
        yes, r2_es, pes = fit_ses(y_tr); cands.append(("SES", yes, r2_es, pes))
    # MA
    if allow["ma"]:
        yma = moving_average(y_tr, allow["ma_win"])
        ss_res = float(np.sum((y_tr - yma)**2)); ss_tot = float(np.sum((y_tr - y_tr.mean())**2))
        r2_ma = 1 - ss_res/ss_tot if ss_tot>0 else 0.0
        cands.append((f"MA({allow['ma_win']})", yma, r2_ma, {"k": allow["ma_win"]}))

    # criterio de selecci√≥n (r o R2, se define afuera)
    return cands

def predict_model(name, pars, x_te, x0, y_tr_last):
    if name.startswith("Poly"):
        deg = int(name[-1]); beta = pars["beta"]
        Xte = np.vstack([x_te**k for k in range(deg+1)]).T
        return Xte @ beta
    if name == "Log-lin" and pars.get("ok", False):
        b0, b1 = pars["beta"]; return np.exp(b0 + b1*x_te)
    # SES / MA: extensi√≥n naive (mantener √∫ltimo nivel) para test
    return np.full_like(x_te, y_tr_last, dtype="float64")

def build_segments_from_cuts(df, cut_str, min_days=180):
    cuts = []
    if cut_str.strip():
        for tok in cut_str.split(","):
            try:
                cuts.append(pd.to_datetime(tok.strip()).normalize())
            except Exception:
                pass
    cuts = [c for c in sorted(cuts) if df["date"].min() < c < df["date"].max()]
    segs, start = [], df["date"].min().normalize()
    for c in cuts:
        segs.append((start, c))
        start = c + pd.Timedelta(days=1)
    segs.append((start, df["date"].max().normalize()))
    segs = [s for s in segs if (s[1]-s[0]).days + 1 >= min_days]
    return segs

def evaluate_segments(df, segments, allow, target="r", ma_win=30):
    rows = []
    w_sum = 0.0; r_num = 0.0; r2_num = 0.0
    for (a,b) in segments:
        d = df[(df["date"]>=a) & (df["date"]<=b)].copy().reset_index(drop=True)
        n = len(d)
        if n < 20:  # tramo muy corto, omitir
            continue
        n_tr = max(int(n*0.8), 1)
        d_tr, d_te = d.iloc[:n_tr], d.iloc[n_tr:]
        if len(d_te)==0:
            continue
        x0 = d_tr["date"].iloc[0]
        x_tr = (d_tr["date"] - x0).dt.days.values.astype(float)
        y_tr = d_tr["tcr"].astype("float64").values
        x_te = (d_te["date"] - x0).dt.days.values.astype(float)
        y_te = d_te["tcr"].astype("float64").values

        all_cands = choose_model(x_tr, y_tr, allow)
        # M√©trica de selecci√≥n
        scored = []
        for name, yhat_tr, r2_tr, pars in all_cands:
            r_tr = pearson_r(y_tr, yhat_tr)
            score = r_tr if target == "r" else r2_tr
            scored.append((score, name, yhat_tr, r2_tr, r_tr, pars))
        if not scored:
            continue
        scored.sort(key=lambda z: z[0], reverse=True)
        _, model, yhat_tr, r2_tr, r_tr, pars = scored[0]

        yhat_te = predict_model(model, pars, x_te, x0, y_tr[-1])
        mae, rmse, mape, rmse_pct, jb_p = metrics_test(y_te, yhat_te)

        rows.append(dict(
            tramo=f"{a.date()}‚Äì{b.date()}",
            n=int(n),
            modelo=model,
            R2_train=round(r2_tr*100,2),
            r_train=round(r_tr*100,2),
            MAE=round(mae,5),
            RMSE=round(rmse,5),
            MAPE=round(mape,3),
            RMSEpct=round(rmse_pct,3),
            JB_p=round(jb_p,4),
        ))

        w = n
        w_sum += w
        r2_num += (r2_tr*100)*w
        r_num  += (r_tr*100)*w

    tabla = pd.DataFrame(rows)
    R2_prom = float(r2_num/w_sum) if w_sum>0 else float("nan")
    r_prom  = float(r_num /w_sum) if w_sum>0 else float("nan")
    return tabla, R2_prom, r_prom

# ---------- UI ----------
col = st.columns([2,1,1,1])
with col[0]:
    default_cuts = "2004-12-31, 2016-12-31, 2019-12-31"
    cut_str = st.text_input("Fechas de corte (YYYY-MM-DD, separadas por coma)", value=default_cuts)
with col[1]:
    min_days = st.number_input("M√≠n. d√≠as por tramo", min_value=120, max_value=2000, value=180, step=30)
with col[2]:
    target_metric = st.selectbox("Meta de ajuste", ["r (correlaci√≥n)", "R2 (determinaci√≥n)"], index=0)
with col[3]:
    ma_win = st.number_input("Ventana MA (d√≠as)", min_value=7, max_value=90, value=30, step=1)

c2 = st.columns(6)
with c2[0]: allow_poly1 = st.checkbox("Poly1", True)
with c2[1]: allow_poly2 = st.checkbox("Poly2", True)
with c2[2]: allow_poly3 = st.checkbox("Poly3", False)
with c2[3]: allow_log   = st.checkbox("Log-lin", True)
with c2[4]: allow_ses   = st.checkbox("SES", True)
with c2[5]: allow_ma    = st.checkbox("MA", True)

if st.button("Calcular tramos y m√©tricas", type="primary"):
    df = hist.copy()
    df["date"] = pd.to_datetime(df["date"])
    df = df.sort_values("date").reset_index(drop=True)

    allow = dict(poly1=allow_poly1, poly2=allow_poly2, poly3=allow_poly3,
                 log=allow_log, ses=allow_ses, ma=allow_ma, ma_win=int(ma_win))
    segments = build_segments_from_cuts(df, cut_str, min_days=min_days)

    if not segments:
        st.warning("No se construyeron tramos (revisa cortes y/o m√≠nimo de d√≠as).")
    else:
        st.write("**Tramos:**", " | ".join([f"{a.date()}‚Äì{b.date()}" for a,b in segments]))
        tabla, R2_prom, r_prom = evaluate_segments(df, segments, allow,
                                                   target=("r" if target_metric.startswith("r") else "R2"),
                                                   ma_win=int(ma_win))
        if tabla.empty:
            st.warning("No hubo suficiente data en los tramos para back-testing.")
        else:
            colm = st.columns(2)
            with colm[0]:
                st.metric("R¬≤ promedio ponderado (entrenamiento)", f"{R2_prom:.2f}%")
            with colm[1]:
                ok = (r_prom >= 95.0) if target_metric.startswith("r") else (R2_prom >= 95.0)
                etiqueta = "‚â•95% ‚úÖ" if ok else "Por debajo de 95%"
                st.metric(("r promedio ponderado (entrenamiento)" if target_metric.startswith("r") else "R¬≤ objetivo"),
                          f"{(r_prom if target_metric.startswith('r') else R2_prom):.2f}%", delta=etiqueta)

            st.info("Objetivo sugerido por consigna: **coeficiente de correlaci√≥n (r) ‚â• 95%** en promedio ponderado; adem√°s, **RMSE% ‚â§ 5** por tramo y residuos con **JB p>0.05** preferentemente.")
            st.dataframe(tabla, use_container_width=True)

            # Descargas
            st.download_button("‚¨áÔ∏è CSV (tramos + m√©tricas)", data=tabla.to_csv(index=False).encode("utf-8"),
                               file_name="tramos_backtesting.csv", mime="text/csv")

            # Bloque LaTeX
            lines = []
            for _,r in tabla.iterrows():
                line = (f"{r['tramo']} & {r['n']} & {r['modelo']} & {r['R2_train']:.2f} & "
                        f"{r['r_train']:.2f} & {r['MAE']:.5f} & {r['RMSE']:.5f} & "
                        f"{r['MAPE']:.2f} & {r['RMSEpct']:.

